{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c81e46-6b07-4db9-8d8c-855a8e93f5e0",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4b5df9fe-d8bf-42e3-afbc-a093f28e4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='jchavezar-demo'\n",
    "REGION='us-central1'\n",
    "DATASET_NAME='mpg-dataset'\n",
    "DATASET_SOURCE='gs://vtx-datasets-public/auto-mpg.csv'\n",
    "PIPELINE_ROOT_PATH='gs://vtx-root-path'\n",
    "MODEL_EXPORT_GCS_PATH='gs://vtx-models'\n",
    "MODEL_PATH=MODEL_EXPORT_GCS_PATH+'/mpg/model'\n",
    "MODEL_NAME='mpg-pipe-model'\n",
    "ENDPOINT_NAME='mpg-pipe-e'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444c749-ee42-4d5b-92e9-0337d4955f71",
   "metadata": {},
   "source": [
    "## Create pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "095c918b-8407-4499-94f0-55884a15c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import( component, pipeline, Input, Artifact)\n",
    "from google_cloud_pipeline_components import aiplatform\n",
    "from kfp.v2.components import importer_node \n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "@component(packages_to_install=[\n",
    "    'google-cloud-aiplatform', \n",
    "    'pandas', \n",
    "    'gcsfs',\n",
    "    'tensorflow'\n",
    "])\n",
    "def training(\n",
    "    dataset: Input[Artifact], \n",
    "    model_export_gcs: str) -> str:\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    \n",
    "    dataset = aiplatform.TabularDataset('projects/' + dataset.uri.split('projects/')[-1])\n",
    "    dataset = dataset._gca_resource.metadata.get(\"inputConfig\").get(\"gcsSource\").get(\"uri\")[0]\n",
    "    dataset = pd.read_csv(dataset)\n",
    "    dataset = dataset.dropna()\n",
    "    dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "    dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "    train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    train_stats = train_dataset.describe()\n",
    "    train_stats.pop(\"MPG\")\n",
    "    train_stats = train_stats.transpose()\n",
    "\n",
    "    train_labels = train_dataset.pop('MPG')\n",
    "    test_labels = test_dataset.pop('MPG')\n",
    "    def norm(x):\n",
    "        return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "\n",
    "    normed_train_data = norm(train_dataset)\n",
    "    normed_test_data = norm(test_dataset)\n",
    "\n",
    "\n",
    "    def build_model():\n",
    "        model_ai = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "        model_ai.compile(loss='mse',\n",
    "                         optimizer=optimizer,\n",
    "                         metrics=['mae', 'mse'])\n",
    "        return model_ai\n",
    "\n",
    "\n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "    EPOCHS = 1000\n",
    "\n",
    "    # The patience parameter is the amount of epochs to check for improvement\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    early_history = model.fit(normed_train_data, train_labels,\n",
    "                              epochs=EPOCHS, validation_split=0.2,\n",
    "                              callbacks=[early_stop])\n",
    "\n",
    "    # Export model and save to GCS\n",
    "    BUCKET = model_export_gcs\n",
    "    model.save(BUCKET + '/mpg/model')\n",
    "    \n",
    "    return 'test'\n",
    "\n",
    "@pipeline(name='firsttest')\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    dataset_name: str,\n",
    "    dataset_source: str,\n",
    "    model_export_gcs: str,\n",
    "    model_path: str,\n",
    "    model_name: str,\n",
    "    endpoint_name: str\n",
    "):\n",
    "    create_dataset_job = aiplatform.TabularDatasetCreateOp(\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        display_name=dataset_name,\n",
    "        gcs_source=dataset_source,\n",
    "    )\n",
    "    train_job = training(\n",
    "        create_dataset_job.outputs['dataset'], \n",
    "        model_export_gcs)\n",
    "    importer_spec = importer_node.importer(\n",
    "        artifact_uri=model_path, \n",
    "        artifact_class=artifact_types.UnmanagedContainerModel, \n",
    "        metadata={\n",
    "            'containerSpec': {\n",
    "                'imageUri':\n",
    "                'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest'\n",
    "        }\n",
    "    }).after(train_job)\n",
    "    model_upload_job = aiplatform.ModelUploadOp(\n",
    "        project=project_id,\n",
    "        display_name=model_name,\n",
    "        unmanaged_container_model=importer_spec.outputs['artifact']\n",
    "    ).after(importer_spec)\n",
    "    create_endpoint_job = aiplatform.EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        display_name=endpoint_name,\n",
    "    )\n",
    "    model_deploy_job = aiplatform.ModelDeployOp(\n",
    "        model=model_upload_job.outputs['model'],\n",
    "        endpoint=create_endpoint_job.outputs['endpoint'],\n",
    "        traffic_split={\"0\": 100},\n",
    "        dedicated_resources_machine_type='n1-standard-2',\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d435686-6d94-4471-a487-51f2907a1ca6",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "52e97f47-1de0-4d6b-9e0c-784c65fd1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from kfp.v2 import compiler\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='first_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d6965397-1ea2-4788-a160-147018ea8cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/firsttest-20220607015419\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/firsttest-20220607015419')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/firsttest-20220607015419?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"training-v1\",\n",
    "    template_path=\"first_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        'region': REGION,\n",
    "        'dataset_name': DATASET_NAME,\n",
    "        'dataset_source': DATASET_SOURCE,\n",
    "        'model_export_gcs': MODEL_EXPORT_GCS_PATH,\n",
    "        'model_path': MODEL_PATH,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'endpoint_name': ENDPOINT_NAME\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1cc87-56f4-4eae-b2d7-9ff97f20bdda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
