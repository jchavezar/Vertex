{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e050c232-2b23-403a-8440-9e7731b779de",
   "metadata": {},
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "10f1d0c2-49f2-4588-b626-bae5825383bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='jchavezar-demo'\n",
    "REGION='us-central1'\n",
    "DATASET_NAME='mpg-dataset'\n",
    "DATASET_GCS_SOURCE='gs://jchavezar-public-datasets/auto-mpg.csv'\n",
    "PIPELINE_ROOT_PATH='gs://vtx-root-path/'\n",
    "MODEL_DIR='gs://vtx-models/custom-pipe/'\n",
    "MODEL_NAME='mpg-pipe'\n",
    "ENDPOINT_NAME='mpg-endp-pip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5dcab91b-f74d-4ff3-bd0a-807c3112681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.jchavezar-demo.appspot.com/\n",
      "gs://cloud-ai-platform-9b627e36-89f4-4b8c-bf59-1e00936392b5/\n",
      "gs://jchavezar-demo_cloudbuild/\n",
      "gs://vtx-artifacts/\n",
      "gs://vtx-cpr/\n",
      "gs://vtx-datasets-public/\n",
      "gs://vtx-models/\n",
      "gs://vtx-packages/\n",
      "gs://vtx-pipelines/\n",
      "gs://vtx-root-path/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee00684-ffba-456e-b137-fdbd325798a8",
   "metadata": {},
   "source": [
    "## Create Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "485c4fb7-6473-4568-b133-9c361f878d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import (component, Input, Artifact)\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform',\n",
    "        'pandas',\n",
    "        'gcsfs',\n",
    "        'tensorflow'\n",
    "    ])\n",
    "def train(dataset: Input[Artifact], model_dir: str) -> str:\n",
    "    from google.cloud import aiplatform\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    import os\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    dataset = aiplatform.TabularDataset('projects/' + dataset.uri.split('projects/')[-1])\n",
    "    gcs_resource = dataset.gca_resource.metadata['inputConfig']['gcsSource']['uri'][0]\n",
    "    dataset = pd.read_csv(gcs_resource)\n",
    "    \n",
    "    dataset = dataset.dropna()\n",
    "    dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "    dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "\n",
    "    train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "    train_stats = train_dataset.describe()\n",
    "    train_stats.pop(\"MPG\")\n",
    "    train_stats = train_stats.transpose()\n",
    "    \n",
    "    train_labels = train_dataset.pop('MPG')\n",
    "    test_labels = test_dataset.pop('MPG')\n",
    "    \n",
    "    #Normalization\n",
    "    \n",
    "    def norm(x):\n",
    "        return (x - train_stats['mean']) / train_stats['std']\n",
    "    \n",
    "    normed_train_data = norm(train_dataset)\n",
    "    normed_test_data = norm(test_dataset)\n",
    "    \n",
    "    def build_model():\n",
    "        model_ai = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "        model_ai.compile(loss='mse',\n",
    "                     optimizer=optimizer,\n",
    "                     metrics=['mae', 'mse'])\n",
    "        return model_ai\n",
    "    \n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "    EPOCHS = 1000\n",
    "    \n",
    "    # The patience parameter is the amount of epochs to check for improvement\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    early_history = model.fit(normed_train_data, train_labels,\n",
    "                              epochs=EPOCHS, validation_split=0.2,\n",
    "                              callbacks=[early_stop])\n",
    "    # Export model and save to GCS\n",
    "    print(model_dir)\n",
    "    \n",
    "    output_directory = model_dir\n",
    "    if os.environ.get('AIP_MODEL_DIR') is not None:\n",
    "        output_directory = os.environ[\"AIP_MODEL_DIR\"]    \n",
    "    \n",
    "    print(output_directory)\n",
    "    \n",
    "    model.save(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10944e-782f-434a-9f23-7b3985ff0d15",
   "metadata": {},
   "source": [
    "## Create a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ab3540f0-1674-45af-a5b9-dd9bba82c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import pipeline\n",
    "from google_cloud_pipeline_components import aiplatform as gcc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@pipeline(name='mpg-pipe')\n",
    "def pipeline(\n",
    "    display_name_ds: str,\n",
    "    gcs_source: str,\n",
    "    project: str,\n",
    "    model_dir: str,\n",
    "    model_display_name: str,\n",
    "    region: str,\n",
    "    endpoint_name: str\n",
    "):\n",
    "    create_dataset_task = gcc.TabularDatasetCreateOp(\n",
    "        display_name=display_name_ds,\n",
    "        gcs_source=gcs_source,\n",
    "        project=project,\n",
    "    )\n",
    "    custom_train_task = train(\n",
    "        create_dataset_task.output,\n",
    "        model_dir = model_dir,\n",
    "    )\n",
    "    upload_model_task = gcc.ModelUploadOp(\n",
    "        display_name=model_display_name,\n",
    "        project=project,\n",
    "        location=region,\n",
    "        serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest',\n",
    "        artifact_uri=model_dir\n",
    "    )\n",
    "    upload_model_task.after(custom_train_task)\n",
    "    create_endpoint_task = gcc.EndpointCreateOp(\n",
    "        display_name=endpoint_name,\n",
    "        project=project,\n",
    "    )\n",
    "    deploy_model_task = gcc.ModelDeployOp(\n",
    "        endpoint = create_endpoint_task.outputs['endpoint'],\n",
    "        model = upload_model_task.outputs['model'],\n",
    "        traffic_split={'0': 100},\n",
    "        dedicated_resources_machine_type='n1-standard-2',\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f40097-dddf-4f7e-bdd5-d16384dd24f4",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7fc45a1c-0f93-42a0-b2dc-6f4466fa2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='mpg_pipe.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1223f-1873-472b-ae95-02aed29a3512",
   "metadata": {},
   "source": [
    "## Run Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3d6fd686-6957-45c4-b868-1c77382ed230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/569083142710/locations/us-central1/pipelineJobs/mpg-pipe-20220613205603\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/569083142710/locations/us-central1/pipelineJobs/mpg-pipe-20220613205603')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mpg-pipe-20220613205603?project=569083142710\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"mpg-pipe-job\",\n",
    "    template_path=\"mpg_pipe.json\",\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    parameter_values={\n",
    "        'display_name_ds': DATASET_NAME,\n",
    "        'gcs_source': DATASET_GCS_SOURCE,\n",
    "        'project': PROJECT_ID,\n",
    "        'model_dir': MODEL_DIR,\n",
    "        'model_display_name': MODEL_NAME,\n",
    "        'region': REGION,\n",
    "        'endpoint_name': ENDPOINT_NAME\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bab99e-4f9e-4946-a830-1bb657aab1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
